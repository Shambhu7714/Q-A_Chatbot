# ğŸ“˜ PDF Q&A â€” Gemini RAG (Local)

![alt text](image.png)

A simple **Retrieve-and-Generate (RAG)** system using:

âœ… Google Gemini (google-genai)
âœ… FAISS / SQLite / Chroma vector database
âœ… FastAPI backend
âœ… Upload PDF â†’ Chunk â†’ Embed â†’ Store â†’ Ask Questions â†’ Get Answers
âœ… Supports summarization, session history, and secure API usage

---

# ğŸš€ 1. Setup (Windows / Mac / Linux)

### 1ï¸âƒ£ Clone the project

```
git clone https://github.com/Shambhu7714/Q-A_Chatbot.git
cd pdf_gemini_rag
```

---

# ğŸ 2. Create & activate virtual environment

### Windows (PowerShell):

```
python -m venv venv
venv\Scripts\activate
```

### Linux/Mac:

```
python3 -m venv venv
source venv/bin/activate
```

---

# ğŸ“¦ 3. Install dependencies

```
pip install --upgrade pip
pip install -r requirements.txt
```

If FAISS fails on Windows:

```
pip install chromadb
```

---

# ğŸ”‘ 4. Add your Gemini API key

Create a `.env` file in project root:

```
GOOGLE_API_KEY=your_key_here
```

---

# ğŸ” 5. Verify Embedding Works

(Optional but recommended)

```
python test_embed.py
```

Expected:

```
OK embed dims: 768
EMBED TEST OK: True
```

If this fails â†’ embeddings wonâ€™t work â†’ send output to debug.

---

# ğŸ“„ 6. Index a PDF

Place your PDF in project folder (example: `Care_Helth_plains.pdf`).

Then run:

```
python index_pdf_simple.py
```

Expected:

```
Indexed chunks: <number>
```

This step:

* Extracts text from PDF
* Cleans & chunks it
* Generates embeddings
* Stores them in FAISS or Chroma

---

# â“ 7. Ask Questions (CLI Test)

```
![alt text](image-1.png)
python query_simple.py
```

Expected:

```
Retrieved: [0.88, 0.77, ...]
ANSWER:
 <your answer here>
```

---

# ğŸŒ 8. Start FastAPI Server

```
uvicorn app:app --reload --port 8000
```

Open browser:

ğŸ‘‰ [http://localhost:8000/](http://localhost:8000/)

You will see:

* Upload PDF
* Ask Questions
* Summarize
* Session ID
* History

---

# ğŸ”§ 9. Debug Endpoints (Important for fixing issues)

### Available models:

```
GET /debug/models
```

### Stored chunks:

```
GET /debug/pdf/{pdf_id}/chunks
```

### Stored embeddings:

```
GET /debug/pdf/{pdf_id}/embeddings
```

### Retrieval test:

```
GET /debug/retrieve?pdf_id=<id>&q=your+question
```

---

# ğŸ“ Project Structure (High Level)

```
pdf_gemini_rag/
â”‚
â”œâ”€â”€ app.py                   â†’ FastAPI backend
â”œâ”€â”€ embedding_client.py      â†’ Gemini embed & text generation
â”œâ”€â”€ rag_engine.py            â†’ RAG pipeline (retrieve & generate)
â”œâ”€â”€ vector_store_sqlite.py   â†’ SQLite vector DB (or FAISS/Chroma)
â”œâ”€â”€ index_pdf_simple.py      â†’ CLI PDF indexer
â”œâ”€â”€ query_simple.py          â†’ CLI question tester
â”œâ”€â”€ test_embed.py            â†’ Quick SDK embedding test
â”‚
â”œâ”€â”€ static/                  â†’ Frontend JS
â”œâ”€â”€ templates/               â†’ Frontend HTML (Jinja)
â”‚
â”œâ”€â”€ data/                    â†’ Vector DB (FAISS / Chroma)
â”œâ”€â”€ .env                     â†’ Gemini API key
â””â”€â”€ requirements.txt
```

---

# âš ï¸ Important Notes

### âœ” Make sure your Gemini generation model is valid

Use only the models listed in:

```
/debug/models
```

Example good models:

* `"models/gemini-pro"`
* `"models/gemini-1.5-pro"`
* `"models/gemini-1.5-flash"`
* `"text-embedding-004"` (embedding model)

### âœ” Do NOT use `"gemini-2.5-pro"` (vision model, no text output)

---

# â¤ï¸ Tips for Better RAG Answers

* Use **chunk size ~800** and **overlap ~100**
* Use FAISS when possible (better local performance)
* Limit retrieval to **top_k = 4â€“6**
* Keep prompts clean and specific
* If PDF is huge â†’ use staged summarization

---

# âœ” You're Ready!

If anything breaks, run:


and share the output â€” that instantly reveals the real issue.

---
